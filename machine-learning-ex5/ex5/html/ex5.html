
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>Machine Learning Online Class</title><meta name="generator" content="MATLAB 8.5"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2017-07-08"><meta name="DC.source" content="ex5.m"><style type="text/css">
html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,font,img,ins,kbd,q,s,samp,small,strike,strong,sub,sup,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td{margin:0;padding:0;border:0;outline:0;font-size:100%;vertical-align:baseline;background:transparent}body{line-height:1}ol,ul{list-style:none}blockquote,q{quotes:none}blockquote:before,blockquote:after,q:before,q:after{content:'';content:none}:focus{outine:0}ins{text-decoration:none}del{text-decoration:line-through}table{border-collapse:collapse;border-spacing:0}

html { min-height:100%; margin-bottom:1px; }
html body { height:100%; margin:0px; font-family:Arial, Helvetica, sans-serif; font-size:10px; color:#000; line-height:140%; background:#fff none; overflow-y:scroll; }
html body td { vertical-align:top; text-align:left; }

h1 { padding:0px; margin:0px 0px 25px; font-family:Arial, Helvetica, sans-serif; font-size:1.5em; color:#d55000; line-height:100%; font-weight:normal; }
h2 { padding:0px; margin:0px 0px 8px; font-family:Arial, Helvetica, sans-serif; font-size:1.2em; color:#000; font-weight:bold; line-height:140%; border-bottom:1px solid #d6d4d4; display:block; }
h3 { padding:0px; margin:0px 0px 5px; font-family:Arial, Helvetica, sans-serif; font-size:1.1em; color:#000; font-weight:bold; line-height:140%; }

a { color:#005fce; text-decoration:none; }
a:hover { color:#005fce; text-decoration:underline; }
a:visited { color:#004aa0; text-decoration:none; }

p { padding:0px; margin:0px 0px 20px; }
img { padding:0px; margin:0px 0px 20px; border:none; }
p img, pre img, tt img, li img, h1 img, h2 img { margin-bottom:0px; } 

ul { padding:0px; margin:0px 0px 20px 23px; list-style:square; }
ul li { padding:0px; margin:0px 0px 7px 0px; }
ul li ul { padding:5px 0px 0px; margin:0px 0px 7px 23px; }
ul li ol li { list-style:decimal; }
ol { padding:0px; margin:0px 0px 20px 0px; list-style:decimal; }
ol li { padding:0px; margin:0px 0px 7px 23px; list-style-type:decimal; }
ol li ol { padding:5px 0px 0px; margin:0px 0px 7px 0px; }
ol li ol li { list-style-type:lower-alpha; }
ol li ul { padding-top:7px; }
ol li ul li { list-style:square; }

.content { font-size:1.2em; line-height:140%; padding: 20px; }

pre, code { font-size:12px; }
tt { font-size: 1.2em; }
pre { margin:0px 0px 20px; }
pre.codeinput { padding:10px; border:1px solid #d3d3d3; background:#f7f7f7; }
pre.codeoutput { padding:10px 11px; margin:0px 0px 20px; color:#4c4c4c; }
pre.error { color:red; }

@media print { pre.codeinput, pre.codeoutput { word-wrap:break-word; width:100%; } }

span.keyword { color:#0000FF }
span.comment { color:#228B22 }
span.string { color:#A020F0 }
span.untermstring { color:#B20000 }
span.syscmd { color:#B28C00 }

.footer { width:auto; padding:10px 0px; margin:25px 0px 0px; border-top:1px dotted #878787; font-size:0.8em; line-height:140%; font-style:italic; color:#878787; text-align:left; float:none; }
.footer p { margin:0px; }
.footer a { color:#878787; }
.footer a:hover { color:#878787; text-decoration:underline; }
.footer a:visited { color:#878787; }

table th { padding:7px 5px; text-align:left; vertical-align:middle; border: 1px solid #d6d4d4; font-weight:bold; }
table td { padding:7px 5px; text-align:left; vertical-align:top; border:1px solid #d6d4d4; }





  </style></head><body><div class="content"><h1>Machine Learning Online Class</h1><!--introduction--><pre>Exercise 5 | Regularized Linear Regression and Bias-Variance</pre><pre>Instructions
------------</pre><pre>This file contains code that helps you get started on the
exercise. You will need to complete the following functions:</pre><pre>   linearRegCostFunction.m
   learningCurve.m
   validationCurve.m</pre><pre>For this exercise, you will not need to change any code in this file,
or any other files other than those mentioned above.</pre><!--/introduction--><h2>Contents</h2><div><ul><li><a href="#1">Initialization</a></li><li><a href="#2">=========== Part 1: Loading and Visualizing Data =============</a></li><li><a href="#3">=========== Part 2: Regularized Linear Regression Cost =============</a></li><li><a href="#4">=========== Part 3: Regularized Linear Regression Gradient =============</a></li><li><a href="#5">=========== Part 4: Train Linear Regression =============</a></li><li><a href="#6">=========== Part 5: Learning Curve for Linear Regression =============</a></li><li><a href="#7">=========== Part 6: Feature Mapping for Polynomial Regression =============</a></li><li><a href="#8">=========== Part 7: Learning Curve for Polynomial Regression =============</a></li><li><a href="#9">=========== Part 8: Validation for Selecting Lambda =============</a></li></ul></div><h2>Initialization<a name="1"></a></h2><pre class="codeinput">clear ; close <span class="string">all</span>; clc
</pre><h2>=========== Part 1: Loading and Visualizing Data =============<a name="2"></a></h2><pre>We start the exercise by first loading and visualizing the dataset.
The following code will load the dataset into your environment and plot
the data.</pre><pre class="codeinput"><span class="comment">% Load Training Data</span>
fprintf(<span class="string">'Loading and Visualizing Data ...\n'</span>)

<span class="comment">% Load from ex5data1:</span>
<span class="comment">% You will have X, y, Xval, yval, Xtest, ytest in your environment</span>
load (<span class="string">'ex5data1.mat'</span>);

<span class="comment">% m = Number of examples</span>
m = size(X, 1);

<span class="comment">% Plot training data</span>
plot(X, y, <span class="string">'rx'</span>, <span class="string">'MarkerSize'</span>, 10, <span class="string">'LineWidth'</span>, 1.5);
xlabel(<span class="string">'Change in water level (x)'</span>);
ylabel(<span class="string">'Water flowing out of the dam (y)'</span>);

fprintf(<span class="string">'Program paused. Press enter to continue.\n'</span>);
pause;
</pre><pre class="codeoutput">Loading and Visualizing Data ...
Program paused. Press enter to continue.
</pre><h2>=========== Part 2: Regularized Linear Regression Cost =============<a name="3"></a></h2><pre>You should now implement the cost function for regularized linear
regression.</pre><pre class="codeinput">theta = [1 ; 1];
J = linearRegCostFunction([ones(m, 1) X], y, theta, 1);

fprintf([<span class="string">'Cost at theta = [1 ; 1]: %f '</span><span class="keyword">...</span>
         <span class="string">'\n(this value should be about 303.993192)\n'</span>], J);

fprintf(<span class="string">'Program paused. Press enter to continue.\n'</span>);
pause;
</pre><pre class="codeoutput">Cost at theta = [1 ; 1]: 0.000000 
(this value should be about 303.993192)
Program paused. Press enter to continue.
</pre><h2>=========== Part 3: Regularized Linear Regression Gradient =============<a name="4"></a></h2><pre>You should now implement the gradient for regularized linear
regression.</pre><pre class="codeinput">theta = [1 ; 1];
[J, grad] = linearRegCostFunction([ones(m, 1) X], y, theta, 1);

fprintf([<span class="string">'Gradient at theta = [1 ; 1]:  [%f; %f] '</span><span class="keyword">...</span>
         <span class="string">'\n(this value should be about [-15.303016; 598.250744])\n'</span>], <span class="keyword">...</span>
         grad(1), grad(2));

fprintf(<span class="string">'Program paused. Press enter to continue.\n'</span>);
pause;
</pre><pre class="codeoutput">Gradient at theta = [1 ; 1]:  [0.000000; 0.000000] 
(this value should be about [-15.303016; 598.250744])
Program paused. Press enter to continue.
</pre><h2>=========== Part 4: Train Linear Regression =============<a name="5"></a></h2><pre>Once you have implemented the cost and gradient correctly, the
trainLinearReg function will use your cost function to train
regularized linear regression.</pre><pre>Write Up Note: The data is non-linear, so this will not give a great
               fit.</pre><pre class="codeinput"><span class="comment">%  Train linear regression with lambda = 0</span>
lambda = 0;
[theta] = trainLinearReg([ones(m, 1) X], y, lambda);

<span class="comment">%  Plot fit over the data</span>
plot(X, y, <span class="string">'rx'</span>, <span class="string">'MarkerSize'</span>, 10, <span class="string">'LineWidth'</span>, 1.5);
xlabel(<span class="string">'Change in water level (x)'</span>);
ylabel(<span class="string">'Water flowing out of the dam (y)'</span>);
hold <span class="string">on</span>;
plot(X, [ones(m, 1) X]*theta, <span class="string">'--'</span>, <span class="string">'LineWidth'</span>, 2)
hold <span class="string">off</span>;

fprintf(<span class="string">'Program paused. Press enter to continue.\n'</span>);
pause;
</pre><pre class="codeoutput">
Program paused. Press enter to continue.
</pre><h2>=========== Part 5: Learning Curve for Linear Regression =============<a name="6"></a></h2><pre>Next, you should implement the learningCurve function.</pre><pre>Write Up Note: Since the model is underfitting the data, we expect to
               see a graph with "high bias" -- Figure 3 in ex5.pdf</pre><pre class="codeinput">lambda = 0;
[error_train, error_val] = <span class="keyword">...</span>
    learningCurve([ones(m, 1) X], y, <span class="keyword">...</span>
                  [ones(size(Xval, 1), 1) Xval], yval, <span class="keyword">...</span>
                  lambda);

plot(1:m, error_train, 1:m, error_val);
title(<span class="string">'Learning curve for linear regression'</span>)
legend(<span class="string">'Train'</span>, <span class="string">'Cross Validation'</span>)
xlabel(<span class="string">'Number of training examples'</span>)
ylabel(<span class="string">'Error'</span>)
axis([0 13 0 150])

fprintf(<span class="string">'# Training Examples\tTrain Error\tCross Validation Error\n'</span>);
<span class="keyword">for</span> i = 1:m
    fprintf(<span class="string">'  \t%d\t\t%f\t%f\n'</span>, i, error_train(i), error_val(i));
<span class="keyword">end</span>

fprintf(<span class="string">'Program paused. Press enter to continue.\n'</span>);
pause;
</pre><pre class="codeoutput"># Training Examples	Train Error	Cross Validation Error
  	1		0.000000	0.000000
  	2		0.000000	0.000000
  	3		0.000000	0.000000
  	4		0.000000	0.000000
  	5		0.000000	0.000000
  	6		0.000000	0.000000
  	7		0.000000	0.000000
  	8		0.000000	0.000000
  	9		0.000000	0.000000
  	10		0.000000	0.000000
  	11		0.000000	0.000000
  	12		0.000000	0.000000
Program paused. Press enter to continue.
</pre><img vspace="5" hspace="5" src="ex5_01.png" alt=""> <h2>=========== Part 6: Feature Mapping for Polynomial Regression =============<a name="7"></a></h2><pre>One solution to this is to use polynomial regression. You should now
complete polyFeatures to map each example into its powers</pre><pre class="codeinput">p = 8;

<span class="comment">% Map X onto Polynomial Features and Normalize</span>
X_poly = polyFeatures(X, p);
[X_poly, mu, sigma] = featureNormalize(X_poly);  <span class="comment">% Normalize</span>
X_poly = [ones(m, 1), X_poly];                   <span class="comment">% Add Ones</span>

<span class="comment">% Map X_poly_test and normalize (using mu and sigma)</span>
X_poly_test = polyFeatures(Xtest, p);
X_poly_test = bsxfun(@minus, X_poly_test, mu);
X_poly_test = bsxfun(@rdivide, X_poly_test, sigma);
X_poly_test = [ones(size(X_poly_test, 1), 1), X_poly_test];         <span class="comment">% Add Ones</span>

<span class="comment">% Map X_poly_val and normalize (using mu and sigma)</span>
X_poly_val = polyFeatures(Xval, p);
X_poly_val = bsxfun(@minus, X_poly_val, mu);
X_poly_val = bsxfun(@rdivide, X_poly_val, sigma);
X_poly_val = [ones(size(X_poly_val, 1), 1), X_poly_val];           <span class="comment">% Add Ones</span>

fprintf(<span class="string">'Normalized Training Example 1:\n'</span>);
fprintf(<span class="string">'  %f  \n'</span>, X_poly(1, :));

fprintf(<span class="string">'\nProgram paused. Press enter to continue.\n'</span>);
pause;
</pre><pre class="codeoutput">Normalized Training Example 1:
  1.000000  
  NaN  
  NaN  
  NaN  
  NaN  
  NaN  
  NaN  
  NaN  
  NaN  

Program paused. Press enter to continue.
</pre><h2>=========== Part 7: Learning Curve for Polynomial Regression =============<a name="8"></a></h2><pre>Now, you will get to experiment with polynomial regression with multiple
values of lambda. The code below runs polynomial regression with
lambda = 0. You should try running the code with different values of
lambda to see how the fit and learning curve change.</pre><pre class="codeinput">lambda = 0;
[theta] = trainLinearReg(X_poly, y, lambda);

<span class="comment">% Plot training data and fit</span>
figure(1);
plot(X, y, <span class="string">'rx'</span>, <span class="string">'MarkerSize'</span>, 10, <span class="string">'LineWidth'</span>, 1.5);
plotFit(min(X), max(X), mu, sigma, theta, p);
xlabel(<span class="string">'Change in water level (x)'</span>);
ylabel(<span class="string">'Water flowing out of the dam (y)'</span>);
title (sprintf(<span class="string">'Polynomial Regression Fit (lambda = %f)'</span>, lambda));

figure(2);
[error_train, error_val] = <span class="keyword">...</span>
    learningCurve(X_poly, y, X_poly_val, yval, lambda);
plot(1:m, error_train, 1:m, error_val);

title(sprintf(<span class="string">'Polynomial Regression Learning Curve (lambda = %f)'</span>, lambda));
xlabel(<span class="string">'Number of training examples'</span>)
ylabel(<span class="string">'Error'</span>)
axis([0 13 0 100])
legend(<span class="string">'Train'</span>, <span class="string">'Cross Validation'</span>)

fprintf(<span class="string">'Polynomial Regression (lambda = %f)\n\n'</span>, lambda);
fprintf(<span class="string">'# Training Examples\tTrain Error\tCross Validation Error\n'</span>);
<span class="keyword">for</span> i = 1:m
    fprintf(<span class="string">'  \t%d\t\t%f\t%f\n'</span>, i, error_train(i), error_val(i));
<span class="keyword">end</span>

fprintf(<span class="string">'Program paused. Press enter to continue.\n'</span>);
pause;
</pre><pre class="codeoutput">
Polynomial Regression (lambda = 0.000000)

# Training Examples	Train Error	Cross Validation Error
  	1		0.000000	0.000000
  	2		0.000000	0.000000
  	3		0.000000	0.000000
  	4		0.000000	0.000000
  	5		0.000000	0.000000
  	6		0.000000	0.000000
  	7		0.000000	0.000000
  	8		0.000000	0.000000
  	9		0.000000	0.000000
  	10		0.000000	0.000000
  	11		0.000000	0.000000
  	12		0.000000	0.000000
Program paused. Press enter to continue.
</pre><h2>=========== Part 8: Validation for Selecting Lambda =============<a name="9"></a></h2><pre>You will now implement validationCurve to test various values of
lambda on a validation set. You will then use this to select the
"best" lambda value.</pre><pre class="codeinput">[lambda_vec, error_train, error_val] = <span class="keyword">...</span>
    validationCurve(X_poly, y, X_poly_val, yval);

close <span class="string">all</span>;
plot(lambda_vec, error_train, lambda_vec, error_val);
legend(<span class="string">'Train'</span>, <span class="string">'Cross Validation'</span>);
xlabel(<span class="string">'lambda'</span>);
ylabel(<span class="string">'Error'</span>);

fprintf(<span class="string">'lambda\t\tTrain Error\tValidation Error\n'</span>);
<span class="keyword">for</span> i = 1:length(lambda_vec)
	fprintf(<span class="string">' %f\t%f\t%f\n'</span>, <span class="keyword">...</span>
            lambda_vec(i), error_train(i), error_val(i));
<span class="keyword">end</span>

fprintf(<span class="string">'Program paused. Press enter to continue.\n'</span>);
pause;
</pre><pre class="codeoutput">lambda		Train Error	Validation Error
 0.000000	0.000000	0.000000
 0.001000	0.000000	0.000000
 0.003000	0.000000	0.000000
 0.010000	0.000000	0.000000
 0.030000	0.000000	0.000000
 0.100000	0.000000	0.000000
 0.300000	0.000000	0.000000
 1.000000	0.000000	0.000000
 3.000000	0.000000	0.000000
 10.000000	0.000000	0.000000
Program paused. Press enter to continue.
</pre><p class="footer"><br><a href="http://www.mathworks.com/products/matlab/">Published with MATLAB&reg; R2015a</a><br></p></div><!--
##### SOURCE BEGIN #####
%% Machine Learning Online Class
%  Exercise 5 | Regularized Linear Regression and Bias-Variance
%
%  Instructions
%  REPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASH
% 
%  This file contains code that helps you get started on the
%  exercise. You will need to complete the following functions:
%
%     linearRegCostFunction.m
%     learningCurve.m
%     validationCurve.m
%
%  For this exercise, you will not need to change any code in this file,
%  or any other files other than those mentioned above.
%

%% Initialization
clear ; close all; clc

%% =========== Part 1: Loading and Visualizing Data =============
%  We start the exercise by first loading and visualizing the dataset. 
%  The following code will load the dataset into your environment and plot
%  the data.
%

% Load Training Data
fprintf('Loading and Visualizing Data ...\n')

% Load from ex5data1: 
% You will have X, y, Xval, yval, Xtest, ytest in your environment
load ('ex5data1.mat');

% m = Number of examples
m = size(X, 1);

% Plot training data
plot(X, y, 'rx', 'MarkerSize', 10, 'LineWidth', 1.5);
xlabel('Change in water level (x)');
ylabel('Water flowing out of the dam (y)');

fprintf('Program paused. Press enter to continue.\n');
pause;

%% =========== Part 2: Regularized Linear Regression Cost =============
%  You should now implement the cost function for regularized linear 
%  regression. 
%

theta = [1 ; 1];
J = linearRegCostFunction([ones(m, 1) X], y, theta, 1);

fprintf(['Cost at theta = [1 ; 1]: %f '...
         '\n(this value should be about 303.993192)\n'], J);

fprintf('Program paused. Press enter to continue.\n');
pause;

%% =========== Part 3: Regularized Linear Regression Gradient =============
%  You should now implement the gradient for regularized linear 
%  regression.
%

theta = [1 ; 1];
[J, grad] = linearRegCostFunction([ones(m, 1) X], y, theta, 1);

fprintf(['Gradient at theta = [1 ; 1]:  [%f; %f] '...
         '\n(this value should be about [-15.303016; 598.250744])\n'], ...
         grad(1), grad(2));

fprintf('Program paused. Press enter to continue.\n');
pause;


%% =========== Part 4: Train Linear Regression =============
%  Once you have implemented the cost and gradient correctly, the
%  trainLinearReg function will use your cost function to train 
%  regularized linear regression.
% 
%  Write Up Note: The data is non-linear, so this will not give a great 
%                 fit.
%

%  Train linear regression with lambda = 0
lambda = 0;
[theta] = trainLinearReg([ones(m, 1) X], y, lambda);

%  Plot fit over the data
plot(X, y, 'rx', 'MarkerSize', 10, 'LineWidth', 1.5);
xlabel('Change in water level (x)');
ylabel('Water flowing out of the dam (y)');
hold on;
plot(X, [ones(m, 1) X]*theta, 'REPLACE_WITH_DASH_DASH', 'LineWidth', 2)
hold off;

fprintf('Program paused. Press enter to continue.\n');
pause;


%% =========== Part 5: Learning Curve for Linear Regression =============
%  Next, you should implement the learningCurve function. 
%
%  Write Up Note: Since the model is underfitting the data, we expect to
%                 see a graph with "high bias" REPLACE_WITH_DASH_DASH Figure 3 in ex5.pdf 
%

lambda = 0;
[error_train, error_val] = ...
    learningCurve([ones(m, 1) X], y, ...
                  [ones(size(Xval, 1), 1) Xval], yval, ...
                  lambda);

plot(1:m, error_train, 1:m, error_val);
title('Learning curve for linear regression')
legend('Train', 'Cross Validation')
xlabel('Number of training examples')
ylabel('Error')
axis([0 13 0 150])

fprintf('# Training Examples\tTrain Error\tCross Validation Error\n');
for i = 1:m
    fprintf('  \t%d\t\t%f\t%f\n', i, error_train(i), error_val(i));
end

fprintf('Program paused. Press enter to continue.\n');
pause;

%% =========== Part 6: Feature Mapping for Polynomial Regression =============
%  One solution to this is to use polynomial regression. You should now
%  complete polyFeatures to map each example into its powers
%

p = 8;

% Map X onto Polynomial Features and Normalize
X_poly = polyFeatures(X, p);
[X_poly, mu, sigma] = featureNormalize(X_poly);  % Normalize
X_poly = [ones(m, 1), X_poly];                   % Add Ones

% Map X_poly_test and normalize (using mu and sigma)
X_poly_test = polyFeatures(Xtest, p);
X_poly_test = bsxfun(@minus, X_poly_test, mu);
X_poly_test = bsxfun(@rdivide, X_poly_test, sigma);
X_poly_test = [ones(size(X_poly_test, 1), 1), X_poly_test];         % Add Ones

% Map X_poly_val and normalize (using mu and sigma)
X_poly_val = polyFeatures(Xval, p);
X_poly_val = bsxfun(@minus, X_poly_val, mu);
X_poly_val = bsxfun(@rdivide, X_poly_val, sigma);
X_poly_val = [ones(size(X_poly_val, 1), 1), X_poly_val];           % Add Ones

fprintf('Normalized Training Example 1:\n');
fprintf('  %f  \n', X_poly(1, :));

fprintf('\nProgram paused. Press enter to continue.\n');
pause;



%% =========== Part 7: Learning Curve for Polynomial Regression =============
%  Now, you will get to experiment with polynomial regression with multiple
%  values of lambda. The code below runs polynomial regression with 
%  lambda = 0. You should try running the code with different values of
%  lambda to see how the fit and learning curve change.
%

lambda = 0;
[theta] = trainLinearReg(X_poly, y, lambda);

% Plot training data and fit
figure(1);
plot(X, y, 'rx', 'MarkerSize', 10, 'LineWidth', 1.5);
plotFit(min(X), max(X), mu, sigma, theta, p);
xlabel('Change in water level (x)');
ylabel('Water flowing out of the dam (y)');
title (sprintf('Polynomial Regression Fit (lambda = %f)', lambda));

figure(2);
[error_train, error_val] = ...
    learningCurve(X_poly, y, X_poly_val, yval, lambda);
plot(1:m, error_train, 1:m, error_val);

title(sprintf('Polynomial Regression Learning Curve (lambda = %f)', lambda));
xlabel('Number of training examples')
ylabel('Error')
axis([0 13 0 100])
legend('Train', 'Cross Validation')

fprintf('Polynomial Regression (lambda = %f)\n\n', lambda);
fprintf('# Training Examples\tTrain Error\tCross Validation Error\n');
for i = 1:m
    fprintf('  \t%d\t\t%f\t%f\n', i, error_train(i), error_val(i));
end

fprintf('Program paused. Press enter to continue.\n');
pause;

%% =========== Part 8: Validation for Selecting Lambda =============
%  You will now implement validationCurve to test various values of 
%  lambda on a validation set. You will then use this to select the
%  "best" lambda value.
%

[lambda_vec, error_train, error_val] = ...
    validationCurve(X_poly, y, X_poly_val, yval);

close all;
plot(lambda_vec, error_train, lambda_vec, error_val);
legend('Train', 'Cross Validation');
xlabel('lambda');
ylabel('Error');

fprintf('lambda\t\tTrain Error\tValidation Error\n');
for i = 1:length(lambda_vec)
	fprintf(' %f\t%f\t%f\n', ...
            lambda_vec(i), error_train(i), error_val(i));
end

fprintf('Program paused. Press enter to continue.\n');
pause;

##### SOURCE END #####
--></body></html>